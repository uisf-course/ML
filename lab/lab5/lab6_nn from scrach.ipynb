{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c95e3b7e-57db-4225-8af7-85e82676ae33",
   "metadata": {},
   "source": [
    "## Notebook Objectives\n",
    "\n",
    "\n",
    "In this notebook we are going to implement and train a neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f68f50b8-2074-4636-a4ff-d1fcc82c6b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d8a57-2bfd-4221-a999-1dc66351a6ea",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da63486d-1664-4cc5-9c3f-8908d9a6e9bc",
   "metadata": {},
   "source": [
    "### Abstract Layer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e3ad52-bc7e-4cd4-916c-2c385b069a36",
   "metadata": {},
   "source": [
    "The Layer class serves as an abstract base class for all layers in the network. It includes placeholder methods:\n",
    "\n",
    "-forward: The forward pass computes the output of the layer given an input.\n",
    "-backward: The backward pass computes the gradients with respect to the input and parameters.\n",
    "-step: Updates the layer parameters (weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb940f4-9acf-47d1-803e-39be070b28da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(inp)\n",
    "\n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, lr: float) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f875c-08f3-407c-94b4-8bf9b9a9f5fe",
   "metadata": {},
   "source": [
    "## Linear Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49132b5e-051e-45b8-81a3-06938b9645e8",
   "metadata": {},
   "source": [
    "The Linear class implements the fully connected (or dense) layer of a neural network, which performs a linear transformation on the input:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{W} + \\mathbf{b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2f81c-274d-4e7c-be03-a477c4b7da84",
   "metadata": {},
   "source": [
    "**Initialization**\n",
    "- `self.w`: Represents the weight matrix of shape `(in_dim, out_dim)`, initialized using small random values.\n",
    "- `self.b`: Bias vector of shape `(1, out_dim)`, initialized to zeros.\n",
    "- `self.dw` and `self.db`: These store the computed gradients of weights and biases during backpropagation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Forward Pass**\n",
    "- The forward pass computes:\n",
    "$$\\mathbf{out} = \\mathbf{inp} \\cdot \\mathbf{W} + \\mathbf{b}$$\n",
    "where:\n",
    "  - `inp`: Input matrix of shape `(batch_size, in_dim)`\n",
    "  - `self.w`: Weight matrix of shape `(in_dim, out_dim)`\n",
    "  -\t`self.b`: Bias matrix of shape `(1, out_dim)`\n",
    "-\tThe result is a matrix out of shape `(batch_size, out_dim)`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Backward Pass**\n",
    "- The backward pass computes gradients needed for updating the weights and biases. Given the upstream gradient `up_grad` (from the loss with respect to the output of this layer), we calculate:\n",
    "  - Gradient w.r.t. weights (`self.dw`):\n",
    "    $$\\frac{\\partial L}{\\partial W} = \\mathbf{inp}^T\\cdot\\text{up\\_grad}$$\n",
    "  - Gradient w.r.t. biases (`self.db`):\n",
    "    $$\\frac{\\partial L}{\\partial b} = \\sum \\text{up\\_grad} \\text{ (summed across batch)}$$\n",
    "  - Gradient to propagate to the previous layer (`down_grad`):\n",
    "    $$\\text{down\\_grad} = \\text{up\\_grad} \\cdot W^T$$\n",
    "- This allows the gradient to flow backward to earlier layers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Step Method**\n",
    "- Updates the weights and biases using the computed gradients and learning rate (`lr`):\n",
    "    $$W = W - lr \\cdot \\frac{\\partial L}{\\partial W}$$\n",
    "    $$b = b - lr \\cdot \\frac{\\partial L}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7998bbc-d128-422a-9580-7d3da25b3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer) :\n",
    "    def __init__(self, in_dim, out_dim) :\n",
    "        #Xiavier initialization\n",
    "        pass\n",
    "        \n",
    "    def forward(self, inp:np.ndarray) -> np.ndarray :\n",
    "        \"\"\"Perform the linear transformation: output = inp * W + b\"\"\"\n",
    "        pass\n",
    "\n",
    "    def backward(self, up_grade:np.ndarray)-> np.ndarray :\n",
    "        pass\n",
    "\n",
    "    def step(self, lr:float) -> None :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1596a7a-eefd-411d-b3ea-7c21272f3ccb",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17039109-b496-4483-bc22-e06f257ce949",
   "metadata": {},
   "source": [
    "We can implement activation functions as layers. This will simplify the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c273985-c8a9-4edf-9e60-a42e45750712",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "- The Sigmoid function is defined as follows:\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "- Sigmoid squashes the input into the range [0, 1], making it useful for binary classification tasks.\n",
    "- It converts any real-valued number into a probability-like output.\n",
    "- However, in deeper networks, it may cause vanishing gradients due to its flat slope for extreme values.\n",
    "- The derivative of Sigmoid is convenient to compute using its output  $f(x)$:\n",
    "$$f'(x) = \\frac{-e^{-x}}{(1 + e^{-x})^2} = \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}} = f(x) \\cdot (1-f(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7a492a3-b84e-43fc-8ffd-14de45dd54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer) :\n",
    "    def forward(self, inp:np.ndarray) -> np.ndarray :\n",
    "        pass\n",
    "    def backward(self, up_grad : np.ndarray) -> np.ndarray :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c334bc45-5f15-4ba1-99c1-43ac29a6130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Layer) :\n",
    "    def forward(self, inp) :\n",
    "        self.inp = inp\n",
    "        out = np.maximum(0, inp) # maximum performs element-wise maximum \n",
    "        return out\n",
    "    def backward(self, up_grad) :\n",
    "        return up_grad * (self.inp > 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3414ec-803a-4000-8422-3417fde7c2b0",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5129d8ba-07c4-4023-9792-9490496adae8",
   "metadata": {},
   "source": [
    "### Abstract Loss Class\n",
    "\n",
    "The `Loss` class serves as an abstract base class for all layers in the network. It includes placeholder methods:\n",
    "- `forward`: To compute the loss given predictions and targets.\n",
    "- `backward`: To compute the loss given predictions and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0961720e-fad3-4f23-a602-ca873d8c2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self):\n",
    "        self.prediction = None\n",
    "        self.target = None\n",
    "        self.loss = None\n",
    "\n",
    "    def __call__(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        return self.forward(prediction, target)\n",
    "\n",
    "    def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self) -> np.ndarray:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979cbc0-5963-469a-9844-d11d04f0d5a6",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE) Loss\n",
    "\n",
    "MSE is used primarily for regression tasks, where you need to measure the distance between the predicted continuous values and the true values:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i} (p_i - y_i)^2$$\n",
    "\n",
    "where $p_i$ is the predicted value, $y_i$ is the true value (target) and $N$ is the batch size.\n",
    "\n",
    "The gradient measures the difference between the prediction and the target, scaled by the batch size:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial p_i} = \\frac{2}{N} (p_i - y_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b75a1f1c-6629-40b9-9b5a-68fe15e9eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Loss) :\n",
    "    def forward(self, prediction, target) :\n",
    "        pass\n",
    "    def backward(self) :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd07cb-44b0-4b5d-b429-a1b59ad0aa8a",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ae071-a1d1-4d8a-96a0-d0ddff3cdd51",
   "metadata": {},
   "source": [
    "Now we can combine everything we've done earlier to build a neural network class called `MLP` with the following methods:\n",
    "\n",
    "- `forward`: Sequentially passes input through each layer in the network to compute the output.\n",
    "- `loss`: Computes the loss between the predicted output and the true target using the specified loss function.\n",
    "- `backward`: Propagates the gradient from the loss function through each layer, updating the gradients of the parameters in each layer.\n",
    "- `update`: Updates each layer's parameters (e.g., weights and biases) using the gradients computed during backpropagation.\n",
    "- `train`: Executes the training loop for a specified number of epochs, iterating over the dataset in mini-batches, performing the forward pass, computing the loss, backpropagating the gradients, and updating the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1028b4b2-7de1-48ac-843d-43c5ce778b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP :\n",
    "    def __init__ (self, layers:list[Layer], loss_fun:Loss, lr:float) -> None :\n",
    "        \"\"\"\n",
    "        Multi-Layer Perceptron (MLP) class.\n",
    "        Arguments:\n",
    "        - layers: List of layers (e.g., Linear, ReLU, etc.).\n",
    "        - loss_fn: Loss function object (e.g., CrossEntropy, MSE).\n",
    "        - lr: Learning rate.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.loss_fun = loss_fun\n",
    "        self.lr = lr\n",
    "\n",
    "    def __call__(self, inp) :\n",
    "        self.forward(inp)\n",
    "\n",
    "    def forward(self, inp) :\n",
    "        pass\n",
    "\n",
    "    def loss(self, prediction, target) :\n",
    "        return self.loss_fun(prediction, target)\n",
    "\n",
    "    def backward(self) :\n",
    "        pass\n",
    "\n",
    "    def update(self) :\n",
    "        pass\n",
    "\n",
    "    def train(self, x_train, y_train, x_test, y_test, epochs, batch_size):\n",
    "        \"\"\"Train the MLP over the given dataset for a number of epochs.\"\"\"\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776af4a2-d9ed-490c-9e65-eb637b27aa21",
   "metadata": {},
   "source": [
    "## Regression training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0635e46-fde4-41d0-9dcf-ef0cfa225fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load California Housing dataset\n",
    "california = fetch_california_housing()\n",
    "X = california.data\n",
    "y = california.target.reshape(-1, 1)\n",
    "\n",
    "                              # Normalize features and targets\n",
    "scaler_X = StandardScaler() # a preprocessing tool from scikit-learn that standardizes features by removing the mean and scaling to unit variance.\n",
    "scaler_y = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_scaled, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3bc2f2eb-8dd2-401a-a938-62cfadd6737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n"
     ]
    }
   ],
   "source": [
    "#define layers\n",
    "\n",
    "#define model\n",
    "\n",
    "\n",
    "#train_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88a72941-b751-4ae0-89b7-61548e9f10de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot losses\n",
    "\n",
    "\n",
    "#plot train and validation loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
